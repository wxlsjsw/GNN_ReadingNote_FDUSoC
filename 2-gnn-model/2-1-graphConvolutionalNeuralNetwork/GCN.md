# Semi-Supervised Clssification with Graph Convolutional Network

## 概述
  
这篇是第一个笔记，作为实验。  
    除去综述那一篇，我的主要工作是围绕着github上面[pytorch geometric](https://github.com/rusty1s/pytorch_geometric)这个项目。这样的主要原因是，在看过综述和几篇模型之后还是觉得对于图网络的模型和原理是认识了，但是对于如何在代码中具体实现，以及具体的图的数据结构是什么样的都不是很清楚。甚至于都不是特别清楚图网络到底能什么？（这个问题在之后的文章里我觉得还是要多聊聊的，有关图网络要解决什么问题，在一些文章里还是有一些分析的）碰巧是这个时候刷到了这个项目，然后就跟着这个项目里涉及的17、18年的20几篇GNN相关的文章来同步理解。这篇_Semi-Supervised Classification with Graph Convolutional Network_是其中的第一篇，主要讲的是GCN，是我看的第一篇明确在讲自己是做spectral domain的图**卷积**的文章。所以从这里开始，最基本的**图卷积**的概念来理解还是比较方便的。    
    在这一篇文章里面，前面简单论述了图卷积自身的基本的原理，并没有太深入的数学部分，直接讲最主要的结论了，这点是比较好的，我们在接下来[模型](##模型)这一部分会仔细推导一下的。值得一提的是，本文中也对于图的结构该如何计算loss进行了简述，这里还是很不错的，我们在[Loss](##Loss)会说明一下。之后要说的点就是实验，实际上对于图网络究竟能解决什么样的问题，我个人觉得还是没有特别大的变化，无论是零几年时候的开篇之作，还是目前对于图网络的这些入门作品，我看到的觉得还是觉得变化不大的，可能是应用这里我读的数量还不够，接下来我觉得还是很有必要讨论一下的。在[实验](##实验)一部分里我们会讨论一下本文的实验，以及我们自己复现的时候做出来的结果和理解。  
    好了，让我们开始吧。  

## 模型

在综述的文章中，我们认识到实际上GNN大都是分成两个部分*propagation step*以及updater。相对来说，updater这部分的意义并不是很大，因为都是一些已有的经验，文章中主要讨论的都是*propagation step*这部分，因为实际上我们如果用类似马尔可夫模型之类的思想去理解图，或者在后面综述篇里面去理解图网络的工作原理的话，如何去用算法模拟出图结构中复杂的关系网络里彼此是如何影响的才是这类算法最美丽的点。那么按照正常的定义的话，GCN的*propagation step*是可以被定义为下式：
$$ H^{(l+1)}=\sigma (\widetilde{D}^{-\frac{1}{2}}\widetilde{A}\widetilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)}) $$  
这里面涉及到一些具体的数学推导，这个公式的由来我们在之后综述部分再去详细推导，但是我们现在可以简单地看一下这个公式，首先这个公式给出了一个迭代的网络传播方式。然后看到$$ \widetilde{D}^{-\frac{1}{2}}\widetilde{A}\widetilde{D}^{-\frac{1}{2}} $$这一部分可以理解为本身图的结构所带来的信息，而$$  D^{(l)} $$则是每一层需要去学习的参数。这里的一个点是，这里就充满了矩阵乘法，而且这里面的两个$$ \widetilde{D} $$矩阵本身是对角线矩阵，那么如何去实现高效的稀疏矩阵乘法以及如何快速地计算出矩阵的幂都可以是我们研究的问题。

## Loss

## 实验

## 总结
